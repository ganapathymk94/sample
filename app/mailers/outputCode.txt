import java.time.Instant;
import java.time.LocalDateTime;
import java.time.ZoneId;
import java.time.format.DateTimeFormatter;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.TopicPartition;
import java.io.File;
import java.io.IOException;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;
import java.util.stream.Collectors;

public class KafkaUtil {
    private Properties configs;
    private String KAFKA_IP;
    private String TOPIC;
    private int PARTITIONS;
    private KafkaConsumer<String, String> defconsumer;
    private String sqlitedbfile;
    private String OUTDIR;
    private String OUTFILE;

    public KafkaUtil(String propfile, String OUTDIR) {
        this.OUTDIR = OUTDIR;
        this.configs = new Properties();
        try {
            this.configs.load(new java.io.FileInputStream(propfile));
        } catch (IOException e) {
            e.printStackTrace();
        }
        this.KAFKA_IP = this.configs.getProperty("kafka.KAFKA_IP");
        this.TOPIC = this.configs.getProperty("kafka.TOPIC");
        this.OUTFILE = this.configs.getProperty("kafka.OUTFILE");
        this.defconsumer = this.initconsumer("troubleshoot");
    }

    private KafkaConsumer<String, String> initconsumer(String group) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, this.KAFKA_IP);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(ConsumerConfig.CONSUMER_TIMEOUT_MS_CONFIG, "1000");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, group);
        KafkaConsumer<String, String> kconsumer = new KafkaConsumer<>(props);
        this.PARTITIONS = kconsumer.partitionsFor(this.TOPIC).size();
        return kconsumer;
    }

    private Map<TopicPartition, Long> offsetsForTimes(Set<TopicPartition> partitions, long timestamp) {
        Map<TopicPartition, Long> response = this.defconsumer.offsetsForTimes(partitions.stream()
                .collect(Collectors.toMap(p -> p, p -> timestamp)));
        Map<TopicPartition, Long> offsets = new HashMap<>();
        for (Map.Entry<TopicPartition, org.apache.kafka.clients.consumer.OffsetAndTimestamp> entry : response.entrySet()) {
            if (entry.getValue() == null) {
                this.defconsumer.assign(List.of(entry.getKey()));
                this.defconsumer.seekToEnd(List.of(entry.getKey()));
                offsets.put(entry.getKey(), this.defconsumer.position(entry.getKey()));
            } else {
                offsets.put(entry.getKey(), entry.getValue().offset());
            }
        }
        return offsets;
    }

    private long getEpoch(String datevar) {
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mm:ss");
        LocalDateTime dateTime = LocalDateTime.parse(datevar, formatter);
        return dateTime.atZone(ZoneId.systemDefault()).toInstant().toEpochMilli() / 1000;
    }

    private Map<Integer, Long> getOffsetList(Map<TopicPartition, Long> offsets) {
        Map<Integer, Long> mydict = new HashMap<>();
        for (Map.Entry<TopicPartition, Long> entry : offsets.entrySet()) {
            mydict.put(entry.getKey().partition(), entry.getValue());
        }
        return mydict;
    }

    public List<Map<Integer, Long>> getOffsets(String start, String end) {
        System.out.printf("Topic:Partition count :: %s::%d%n", this.TOPIC, this.PARTITIONS);
        List<TopicPartition> partitions = new ArrayList<>();
        for (int i = 0; i < this.PARTITIONS; i++) {
            partitions.add(new TopicPartition(this.TOPIC, i));
        }
        Map<TopicPartition, Long> startOffsets = this.offsetsForTimes(new HashSet<>(partitions), this.getEpoch(start));
        Map<TopicPartition, Long> endOffsets = this.offsetsForTimes(new HashSet<>(partitions), this.getEpoch(end));
        return List.of(this.getOffsetList(startOffsets), this.getOffsetList(endOffsets));
    }

    public List<String> consumeMsg(String KAFKA_IP, String TOPIC, Map<Integer, Long> start_offsets, Map<Integer, Long> end_offsets) {
        List<String> messagelist = new ArrayList<>();
        KafkaConsumer<String, String> consumer2 = new KafkaConsumer<>(this.configs);
        for (int i = 0; i < start_offsets.size(); i++) {
            TopicPartition partition = new TopicPartition(this.TOPIC, i);
            long start = start_offsets.get(i);
            long end = end_offsets.get(i);
            consumer2.assign(List.of(partition));
            consumer2.seek(partition, start);
            while (true) {
                ConsumerRecords<String, String> records = consumer2.poll(java.time.Duration.ofMillis(100));
                for (ConsumerRecord<String, String> record : records) {
                    if (record.offset() < end) {
                        messagelist.add(record.value());
                    } else {
                        break;
                    }
                }
                if (records.count() == 0) {
                    break;
                }
            }
        }
        consumer2.close();
        return messagelist;
    }
}

